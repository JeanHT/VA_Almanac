---
title: "Get Bills"
author:
  - "Robert J. McGrath, Ph.D."
  - "Jean Thoensen"
date: "`r Sys.Date()`"
documentclass: article
geometry: margin=1in
fontsize: 11pt
output:
  pdf_document:
    toc: false
    df_print: kable
    fig_caption: false
    number_sections: false
    dev: pdf
    highlight: tango
  html_document:
    theme: default
    self_contained: true
    toc: false
    df_print: kable
    fig_caption: false
    number_sections: false
    smart: true
    dev: svg
---
  
```{r setup, include = FALSE}
# DO NOT ALTER THIS CHUNK
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  fig.width = 5,
  fig.asp = 0.618,
  out.width = "70%",
  dpi = 120,
  fig.align = "center",
  cache = FALSE
)

# Load required packages
library(robotstxt)
library(rvest)    
library(tidyverse)
library(xml2)
#library(stringr)   
#library(rebus)     
#library(lubridate)
#library(qdapRegex)
#library (gsubfn)
#library(dplyr)
```

```{r global_variables}
# Web sites are reorganized, naming conventions change, and pages move.
# Global variables may help make this code easier to maintain in the future.
site_url <- "https://lis.virginia.gov"
```

Responsible scraping of Web pages requires that we examine the robots.txt file to determine if the site restricts access. The LIS robots.txt file doesn't contain any restrictions on directories, nor does it request a crawl delay.

```{r check_robots_txt}
robots_txt <- get_robotstxt(site_url)
robots_txt_parsed <- parse_robotstxt(robots_txt)
robots_txt_parsed
```

```{r create_data_frame}
# Data skeleton for presented and ordered printed 
data <- data.frame(session=factor(),
                   billnum=factor(),
                   billnum_abbrv=factor(),
                   desc=factor(),
                   #offered=factor(),
                   #prefiled=factor(),
                   type=factor(),
                   sponsors=factor(),
                   committees=factor(),
                   text=factor(),
                   stringsAsFactors=FALSE)
```

# First, only for last (top) bills - HB presented and ordered printed  
# First, loop through bill numbers (need to do this for Senate and joint resolutions, etc. as well - loop later)

```{r}
# For development purposes, hardcode the session portion of the URL
# for the 1994 House session. This will be expanded into a function later.
session_url <- "cgi-bin/legp604.exe?941+ful+"
```

There are multiple types of legislation introduced in the General Assembly:
*  House bills
*  House joint resolutions
*  House resolutions
*  Senate bills
*  Senate joint resolutions
*  Senate resolutions

Dr. McGrath's ideas:

*  Loop H and S around J and R
*  Loop through HJ as a class
*  Loop through HR as a class 
*  Loop through SJ as a class
*  Loop through SR as a class

Not all types may be introduced in every session. In addition to the annual session, the General Assembly may also convene one or more special sessions.

```{r}
loop_chambers <- c("H", "S")

for (chamber in loop_chambers) {
  # for(i in 1:1500) {
  for (i in 1:10) {
    # Strip leading zeros from the bill number counter
    i1 <- sprintf('%01d', i)
    
    # Build the bill's URL based on the site and base URLs, chamber, and loop counter
    # Will need to add logic for types of legislation
    bill_url <- paste0(site_url, "/", session_url, chamber, "B", i1) # loop through S too
    
    # Read the bill's Web page
    page_html <- read_html(bill_url)
    
    # Extract the bill text from the <div id="mainC"> HTML tag
    bill_html <- page_html %>%
      html_nodes("#mainC") %>%
      html_text()
    
    # In which year was the bill was presented?
    session <- str_extract(bill_html, "[1-9]+ SESSION")
    
    # If the string length is zero, then there was no session that year,
    # and we should skip to the next
    # What's an example of this?
    if (length(session) == 0) {
      break
    }
    
    # What does this do?
    if (is.na(session) == T) {
      break
    }
    
    # Parse the bill text for useful information
    # This may be a good candidate for a function
    billnum <- str_extract(bill_html, "\\w+ BILL NO. \\d+")
    billnum_abbrv <- paste0(chamber, "B", i1)
    sponsors <- str_extract(bill_html, "Patron.*")
    committees <- str_extract(bill_html, "Referred to.*")
    dump <- str_replace_all(bill_html, "[\r\n]" , "")
    text <- str_extract(bill_html, "Be it.*")
    desc <- str_extract(bill_html, "A BILL.*\\.-")
    type <- "presented and ordered printed"
    t <-
      as.data.frame(cbind(
        session,
        billnum,
        billnum_abbrv,
        desc,
        type,
        sponsors,
        committees,
        text
      ))
    try(data <- rbind(data, t), silent = T)
    
    # Wait 2 seconds before scraping the next Web page to be polite.
    # Helps to avoid the server thinking we're an ill-behaved bot.
    Sys.sleep(2)
  }
}
```

Save the results to a CSV file. This is a temporary option since bills with text larger than 32k will overflow their cells. Creating an SQL database or using XML may be better options.

```{r write_to_csv}
write.csv(data, file = "1994.csv")
```
\newpage
# References
Meissner, Peter. 2018. *robotstxt: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler' Permissions Checker.* R package version 0.6.2. https://CRAN.R-project.org/package=robotstxt

Munzert, Simon. 2014. *Automated Data Collection with R: A Practical Guide to Web Scraping and Texting Mining.* Chichester, West Sussex, United Kingdom: Wiley.

R Core Team. 2019. *R: A language and environment for statistical computing.* R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/