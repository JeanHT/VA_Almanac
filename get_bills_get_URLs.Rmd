---
title: "Get Bills"
author:
  - "Robert J. McGrath, Ph.D."
  - "Jean Thoensen"
date: "`r Sys.Date()`"
documentclass: article
geometry: margin=1in
fontsize: 11pt
output:
  pdf_document:
    toc: false
    df_print: kable
    fig_caption: false
    number_sections: false
    dev: pdf
    highlight: tango
  html_document:
    theme: default
    self_contained: true
    toc: false
    df_print: kable
    fig_caption: false
    number_sections: false
    smart: true
    dev: svg
---
  
```{r setup, include = FALSE}
# DO NOT ALTER THIS CHUNK
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  fig.width = 5,
  fig.asp = 0.618,
  out.width = "70%",
  dpi = 120,
  fig.align = "center",
  cache = FALSE
)

# Load required packages
library(robotstxt)
library(rvest)    
library(tidyverse)
#library(xml2)
#library(rebus)     
#library(lubridate)
#library(qdapRegex)
#library (gsubfn)
```

```{r global_variables}
# Web sites are reorganized, naming conventions change, and pages move.
# Global variables may help make this code easier to maintain in the future.
site_url <- "https://lis.virginia.gov"
```

Responsible scraping of Web pages requires that we examine the robots.txt file to determine if the site restricts access. The LIS robots.txt file doesn't contain any restrictions on directories, nor does it request a crawl delay.

```{r check_robots_txt}
# Add error handling later
robots_txt <- get_robotstxt(site_url)
robots_txt_parsed <- parse_robotstxt(robots_txt)
robots_txt_parsed
```

```{r scrape_LIS_home_page}
# Scrape the LIS home page
# Add error handling later
home_page_html <- read_html(site_url)
              
# Wait a second before scraping the next Web page to be polite.
Sys.sleep(1)
```

Building a list of the General Assembly sessions will enable us to build lists of URLs to traverse to read the text of bills.

```{r get_general_assembly_sessions}
# The home page has a drop down list box labeled, "OTHER SESSIONS." The list
# of General Assembly sessions is enclosed in a <div> HTML tag with id="sLink". 
ga_sessions_html <- home_page_html %>%
  html_node("#sLink") %>%
  html_text()

# Clean up the list of sessions into an iterable list.

# Every line ends with a carriage return/line feed combination, providing
# a convenient way to break the HTML into a list of session names.
ga_sessions <- str_split(ga_sessions_html, "\r\n") %>%
  # Work on the first column in the list (it only has one)  
  `[[`(1) %>%
  # Remove the first two rows, which contain the title and a row of dashes  
  `[`(-1:-2) %>%
  # Sort the list in ascending (chronological) order
  sort(decreasing = FALSE) %>%
  # Remove the empty element, an artifact leftover from str_split
  `[`(-1:-1)
```

Turn the list of sessions into a data frame that can be used to narrow a search for bills by a particular year of range of years, if desired.

```{r build_sessions_data_frame}
# How many sessions are there?
num_ga_sessions = length(ga_sessions)

# Preallocate the data frame since we know its size
ga_sessions_df <- data.frame(session_year = character(num_ga_sessions),
                             session_type = character(num_ga_sessions),
                             session_number = character(num_ga_sessions),
                             session_bill_list_url = character(num_ga_sessions),
                             stringsAsFactors = FALSE)

# Fill in each row of the data frame
for (i in 1:num_ga_sessions) {
  # The year is the first four characters of the session title
  session_year = str_sub(ga_sessions[i], 1, 4)
  
  # Special sessions contain the word 'special' in the title;
  # otherwise, it is a regular session.
  session_type <- if (grepl("special", ga_sessions[i], ignore.case = TRUE)) {
    "Special"
  } else {
    "Regular"
  }
  
  # Session numbers start at 1, beginning with first regular session of each year
  if (session_type == "Regular") {
    session_number <- 1
  } else {
    session_number <- session_number + 1
  }
  
  # The URL for a session's list of bills has the form:
  #     https://lis.virginia.gov/cgi-bin/legp604.exe?YYN+lst+ALL
  #     where YY = last 2 digits of year and N = session number  
  session_bill_list_url <- paste0(site_url,
                                  "/cgi-bin/legp604.exe?",
                                  str_sub(session_year, 3, 4),
                                  session_number,
                                  "+lst+ALL")
  
  # Copy the session information into the data frame
  ga_sessions_df[i,] <- c(session_year,
                          session_type,
                          session_number,
                          session_bill_list_url
                        )
}
```

```{r function_get_introduced_bills}
introduced_bills <- function(input_df) {
  # For debugging
  options(max.print = 9999)

  # Each page of bill URLs will be added to this final list
  final_bill_url_list = list()
  
  # Flag to help navigate pages to get all the bills
  more_bills <- TRUE
    
  # Scrape the session home page
  # Add error handling later
  bill_list_page_html <- read_html(input_df[4])
              
  # Wait a second before scraping the next Web page to be polite.
  Sys.sleep(1)
    
  # Use a while loop since we don't know how many bills there are.
  while (more_bills) {
    # The list of bills is contained in an unordered list of class "linkSect."
    # Get all entries within the list, taking only the bill's URL from the <a> node.
    bill_url_list <- as.list(bill_list_page_html %>%
      html_nodes("ul.linkSect > li > a") %>%
      html_attr("href")
    )

    # Bills introduced in a session are displayed 80 per page. If there
    # are more bills, the word "More..." appears at the bottom of the
    # page as a hyperlink
    # containing "ALL". If the URL of the last entry in the list
    # contains "ALL", then there are more bills. We can't assume that
    # the Web site will always display 80 bills per page.
    
    # Get the last URL from the page so we can use it.
    # It will either be the last bill on the page or "More..."
    last_url <- last(bill_url_list)
    
    # Does the last URL contain "ALL"?
    if (grepl("ALL", last_url)) {
      # Yes, the last URL contains "ALL" so it is the "More..." link
      more_bills <- TRUE
      
      # Append the current list to the final list of URLs,
      # but drop the last two entries. The second to last entry
      # will be duplicated at the beginning of the next page.
      # The last entry is the "More..." link.
      final_bill_url_list <- append(final_bill_url_list, head(bill_url_list, -2))

      # Scrape the next page
      # Add error handling later
      bill_list_page_html <- read_html(paste0(site_url, last_url))
            
      # Wait a second before scraping the next Web page to be polite.
      Sys.sleep(1)
    } else {
      # No, the last URL doesn't contain "ALL" so there are no more bills
      more_bills <- FALSE
      
      # Append the current list to the final list of URLs
      final_bill_url_list <- append(final_bill_url_list, bill_url_list)
    }
  }

  # The scraped URLs are just fragments, not complete URLs.
  # Prepend the site URL to build the complete URL for each bill.
  final_bill_url_list <- paste0(site_url, final_bill_url_list)
}
```

```{r function_select_years}
select_years <- function(year_start, year_end) {
  # Add error handling to validate the arguments
  # Argument 1: YYYY as a character vector
  # Argument 2: YYYY as a character vector

  # Find the rows matching the specified years and keep only the URL column
  years_df <- subset(ga_sessions_df, 
                    ga_sessions_df$session_year >= year_start &
                    ga_sessions_df$session_year <= year_end)
  
  # Find all the bills introduced during those years
  apply(years_df, 1, introduced_bills)
  }
```

The main function of the code is a wrapper for the entire process.

```{r main}
select_years("1994", "1995")
```

We have to figure out how to find the URL for each piece of legislation introduced during a session. The URL format for the full text of a bill that was presented and ordered printed is:

https://lis.virginia.gov/cgi-bin/legp604.exe?YYN+ful+CTN

*  YY = Last two digits of year
*  C  = Chamber
*  T = Type of legislation
*  N  = Bill number (1 to 4 digits)

```{r create_data_frame}
# Data skeleton for presented and ordered printed 
#data <- data.frame(session=factor(),
#                   billnum=factor(),
#                   billnum_abbrv=factor(),
#                   desc=factor(),
#                   #offered=factor(),
#                   #prefiled=factor(),
#                   type=factor(),
#                   sponsors=factor(),
#                   committees=factor(),
#                   text=factor(),
#                   stringsAsFactors=FALSE)
```

First, only for last (top) bills - HB presented and ordered printed  
First, loop through bill numbers (need to do this for Senate and joint resolutions, etc. as well - loop later)

There are multiple types of legislation introduced in the General Assembly:
*  House bills
*  House joint resolutions
*  House resolutions
*  Senate bills
*  Senate joint resolutions
*  Senate resolutions

Dr. McGrath's ideas:

*  Loop H and S around J and R
*  Loop through HJ as a class
*  Loop through HR as a class 
*  Loop through SJ as a class
*  Loop through SR as a class

Not all types may be introduced in every session. In addition to the annual session, the General Assembly may also convene one or more special sessions.

HTML bold and italic tags in the bill text confuse things. Normally, the Unicode section character, U+00A7, seems to be handled without difficulty, but ends up as the Unicode replacement character, U+FFFD, if also bolded or italicized. Should bold and italic tags be stripped before parsing out the desired elements?

```{r}
#loop_chambers <- c("H", "S")
#session_url <- "cgi-bin/legp604.exe?941+ful+"

#for (chamber in loop_chambers) {
#  for(i in 1:1500) {
    # Strip leading zeros from the bill number counter
    #i1 <- sprintf('%01d', i)
    
    # Build the bill's URL based on the site and base URLs, chamber, and loop counter
    # Will need to add logic for types of legislation
    #bill_url <- paste0(site_url, "/", session_url, chamber, "B", i1) # loop through S too
    
    # Read the bill's Web page
    #page_html <- read_html(bill_url)
    
    # Extract the bill text from the <div id="mainC"> HTML tag
    #bill_html <- page_html %>%
    #  html_nodes("#mainC") %>%
    #  html_text()
    
    # In which year was the bill was presented?
    #session <- str_extract(bill_html, "[1-9]+ SESSION")
    
    # If the string length is zero, then there was no session that year,
    # and we should skip to the next
    # What's an example of this?
    #if (length(session) == 0) {
    #  break
    #}
    
    # What does this do?
    #if (is.na(session) == T) {
    #  break
    #}
    
    # Parse the bill text for useful information
    # This may be a good candidate for a function
    
    # \w+ matches one or more "word" characters, including
    # alphabetic characters, marks, and decimal number
    # \d+ matches one or more digits
    #billnum <- str_extract(bill_html, "\\w+ BILL NO. \\d+")
    
    #billnum_abbrv <- paste0(chamber, "B", i1)
    
    # Text can say "Patron" or "Patrons"
    # Why extract the word itself?
    #sponsors <- str_extract(bill_html, "Patron.*")
    
    #committees <- str_extract(bill_html, "Referred to.*")
    
    # Strip carriage return/new line combinations
    # This may too aggressive. The output has many
    # occurrences where there is no space between words.
    #bill_html <- str_replace_all(bill_html, "[\r\n]", "")
    
    #text <- str_extract(bill_html, "Be it.*")
    
    # What does this regular expression do?
    # Account for bills vs. resolutions vs. joint resolutions?
    #desc <- str_extract(bill_html, "A BILL.*\\.-")
    
    # What will the source of this information be?
    #type <- "presented and ordered printed"
    
    # Put the extracted data into a data frame
    #t <-
    #  as.data.frame(cbind(
    #    session,
    #    billnum,
    #    billnum_abbrv,
    #    desc,
    #    type,
    #    sponsors,
    #    committees,
    #    text
    #  ))
    
    # Append the new data frame to the master data frame.
    # This is very slow because R must allocate space for a
    # new copy of the data frame and copy everything into it.
    #try(data <- rbind(data, t), silent = T)
    
    # Wait 1 second before scraping the next Web page to be polite.
    # Helps to avoid the server thinking we're an ill-behaved bot.
#    Sys.sleep(1)
#  }
#}
```

Save the results to a CSV file. This is a temporary option since bills with text larger than 32k will overflow their cells. Creating an SQL database or using XML may be better options. Consider packages for writing Excel files instead of CSV.

```{r write_to_csv}
#write.csv(data, file = "1994.csv")
```
\newpage
# References
Grolemund, Garrett, and Hadley Wickham. 2016. *R for Data Science.* Sebastapol, California: O'Reilly Media.

Meissner, Peter. 2018. *robotstxt: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler' Permissions Checker.* R package version 0.6.2. https://CRAN.R-project.org/package=robotstxt

Munzert, Simon. 2014. *Automated Data Collection with R: A Practical Guide to Web Scraping and Texting Mining.* Chichester, West Sussex, United Kingdom: Wiley.

R Core Team. 2019. *R: A language and environment for statistical computing.* R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/

Wickham, Hadley. 2019.*Advanced R, Second Edition.* Chapman and Hall/CRC.